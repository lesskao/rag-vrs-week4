"""
æ€§èƒ½è¯„æµ‹è„šæœ¬
"""
import sys
import os

# èŽ·å–é¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import time
from typing import List, Dict
from src.vectorstores import MilvusVectorStore, QdrantVectorStore, ChromaVectorStore
from src.rag_engine import AdvancedRAGEngine
from src.core.models import Document, QueryRequest
from src.core.config import settings
import json


class RAGBenchmark:
    """RAG ç³»ç»Ÿæ€§èƒ½è¯„æµ‹"""
    
    def __init__(self, num_documents: int = 1000):
        self.num_documents = num_documents
        self.test_documents = self._generate_test_documents()
        self.test_queries = [
            "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
            "å¦‚ä½•ä½¿ç”¨ Python è¿›è¡Œæ•°æ®åˆ†æžï¼Ÿ",
            "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨æœ‰å“ªäº›ï¼Ÿ",
            "æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ",
            "è‡ªç„¶è¯­è¨€å¤„ç†çš„ä¸»è¦æŠ€æœ¯æœ‰å“ªäº›ï¼Ÿ"
        ]
    
    def _generate_test_documents(self) -> List[Document]:
        """ç”Ÿæˆæµ‹è¯•æ–‡æ¡£"""
        print(f"\nç”Ÿæˆ {self.num_documents} æ¡æµ‹è¯•æ–‡æ¡£...")
        
        # ç¤ºä¾‹æ–‡æ¡£æ¨¡æ¿
        tech_topics = [
            ("æ·±åº¦å­¦ä¹ ", "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥žç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„è¡¨ç¤ºã€‚æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚å¸¸è§çš„æ·±åº¦å­¦ä¹ æ¡†æž¶åŒ…æ‹¬ TensorFlowã€PyTorch å’Œ Kerasã€‚"),
            ("æ•°æ®åˆ†æž", "æ•°æ®åˆ†æžæ˜¯æŒ‡é€šè¿‡ç»Ÿè®¡å’Œè®¡ç®—æ–¹æ³•æ¥æ£€æŸ¥ã€æ¸…ç†ã€è½¬æ¢å’Œå»ºæ¨¡æ•°æ®çš„è¿‡ç¨‹ã€‚Python æ˜¯æ•°æ®åˆ†æžçš„çƒ­é—¨è¯­è¨€ï¼Œå¸¸ç”¨çš„åº“åŒ…æ‹¬ Pandasã€NumPy å’Œ Matplotlibã€‚æ•°æ®åˆ†æžå¯ä»¥å¸®åŠ©ä¼ä¸šåšå‡ºæ›´å¥½çš„å†³ç­–ã€‚"),
            ("äººå·¥æ™ºèƒ½", "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚AI æŠ€æœ¯åŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ã€‚AI åœ¨åŒ»ç–—ã€é‡‘èžã€æ•™è‚²ç­‰å¤šä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚"),
            ("æœºå™¨å­¦ä¹ ", "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»Žæ•°æ®ä¸­å­¦ä¹ è€Œæ— éœ€æ˜Žç¡®ç¼–ç¨‹ã€‚æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥åˆ†ä¸ºç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚å¸¸è§çš„åº”ç”¨åŒ…æ‹¬æŽ¨èç³»ç»Ÿã€æ¬ºè¯ˆæ£€æµ‹å’Œé¢„æµ‹åˆ†æžã€‚"),
            ("è‡ªç„¶è¯­è¨€å¤„ç†", "è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½å’Œè¯­è¨€å­¦çš„äº¤å‰é¢†åŸŸï¼Œä¸“æ³¨äºŽè®©è®¡ç®—æœºç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚NLP æŠ€æœ¯åŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æžã€æœºå™¨ç¿»è¯‘å’Œé—®ç­”ç³»ç»Ÿã€‚BERT å’Œ GPT æ˜¯æµè¡Œçš„ NLP æ¨¡åž‹ã€‚"),
            ("è®¡ç®—æœºè§†è§‰", "è®¡ç®—æœºè§†è§‰æ˜¯ä½¿è®¡ç®—æœºèƒ½å¤Ÿä»Žå›¾åƒæˆ–è§†é¢‘ä¸­èŽ·å–é«˜çº§ç†è§£çš„é¢†åŸŸã€‚å®ƒåŒ…æ‹¬å›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡ã€‚å·ç§¯ç¥žç»ç½‘ç»œï¼ˆCNNï¼‰æ˜¯è®¡ç®—æœºè§†è§‰ä¸­æœ€å¸¸ç”¨çš„æž¶æž„ã€‚åº”ç”¨åŒ…æ‹¬è‡ªåŠ¨é©¾é©¶å’Œäººè„¸è¯†åˆ«ã€‚"),
            ("äº‘è®¡ç®—", "äº‘è®¡ç®—æ˜¯é€šè¿‡äº’è”ç½‘æä¾›è®¡ç®—èµ„æºå’ŒæœåŠ¡çš„æ¨¡åž‹ã€‚ä¸»è¦çš„äº‘æœåŠ¡æä¾›å•†åŒ…æ‹¬ AWSã€Azure å’Œ Google Cloudã€‚äº‘è®¡ç®—æä¾›äº†å¯æ‰©å±•æ€§ã€çµæ´»æ€§å’Œæˆæœ¬æ•ˆç›Šï¼Œæ˜¯çŽ°ä»£åº”ç”¨å¼€å‘çš„åŸºç¡€è®¾æ–½ã€‚"),
            ("å¤§æ•°æ®", "å¤§æ•°æ®æŒ‡çš„æ˜¯ä¼ ç»Ÿæ•°æ®å¤„ç†è½¯ä»¶æ— æ³•åœ¨åˆç†æ—¶é—´å†…å¤„ç†çš„è¶…å¤§è§„æ¨¡æ•°æ®é›†ã€‚å¤§æ•°æ®æŠ€æœ¯åŒ…æ‹¬ Hadoopã€Spark å’Œ NoSQL æ•°æ®åº“ã€‚å¤§æ•°æ®åˆ†æžå¯ä»¥æ­ç¤ºéšè—çš„æ¨¡å¼å’Œè¶‹åŠ¿ï¼Œä¸ºä¸šåŠ¡æä¾›æ´žå¯Ÿã€‚"),
            ("åŒºå—é“¾", "åŒºå—é“¾æ˜¯ä¸€ç§åˆ†å¸ƒå¼è´¦æœ¬æŠ€æœ¯ï¼Œå¯ä»¥å®‰å…¨åœ°è®°å½•äº¤æ˜“ã€‚æ¯ä¸ªåŒºå—åŒ…å«ä¸€ç»„äº¤æ˜“ï¼Œå¹¶é€šè¿‡åŠ å¯†å“ˆå¸Œé“¾æŽ¥åˆ°å‰ä¸€ä¸ªåŒºå—ã€‚åŒºå—é“¾æŠ€æœ¯è¢«åº”ç”¨äºŽåŠ å¯†è´§å¸ã€ä¾›åº”é“¾ç®¡ç†å’Œæ™ºèƒ½åˆçº¦ç­‰é¢†åŸŸã€‚"),
            ("ç‰©è”ç½‘", "ç‰©è”ç½‘ï¼ˆIoTï¼‰æ˜¯æŒ‡é€šè¿‡äº’è”ç½‘è¿žæŽ¥çš„ç‰©ç†è®¾å¤‡ç½‘ç»œã€‚è¿™äº›è®¾å¤‡å¯ä»¥æ”¶é›†å’Œäº¤æ¢æ•°æ®ï¼Œå®žçŽ°æ™ºèƒ½å®¶å±…ã€å·¥ä¸šè‡ªåŠ¨åŒ–å’Œæ™ºæ…§åŸŽå¸‚ç­‰åº”ç”¨ã€‚IoT è®¾å¤‡é€šå¸¸é…å¤‡ä¼ æ„Ÿå™¨å’Œæ‰§è¡Œå™¨ã€‚")
        ]
        
        documents = []
        for i in range(self.num_documents):
            topic, content = tech_topics[i % len(tech_topics)]
            
            # æ·»åŠ ä¸€äº›å˜åŒ–
            doc_content = f"{content} è¿™æ˜¯ç¬¬ {i+1} æ¡æ–‡æ¡£çš„è¡¥å……ä¿¡æ¯ã€‚"
            
            doc = Document(
                id=f"doc_{i:04d}",
                content=doc_content,
                metadata={
                    "category": "tech",
                    "topic": topic,
                    "index": i,
                    "source": "benchmark"
                }
            )
            documents.append(doc)
        
        print(f"âœ“ æˆåŠŸç”Ÿæˆ {len(documents)} æ¡æ–‡æ¡£")
        return documents
    
    def benchmark_vector_store(
        self,
        store_name: str,
        vector_store
    ) -> Dict:
        """
        è¯„æµ‹å•ä¸ªå‘é‡æ•°æ®åº“
        
        Args:
            store_name: æ•°æ®åº“åç§°
            vector_store: å‘é‡æ•°æ®åº“å®žä¾‹
        
        Returns:
            è¯„æµ‹ç»“æžœå­—å…¸
        """
        print(f"\n{'='*60}")
        print(f"è¯„æµ‹: {store_name}")
        print(f"{'='*60}")
        
        results = {
            "name": store_name,
            "index_time": 0,
            "search_time": 0,
            "avg_search_time": 0,
            "errors": []
        }
        
        try:
            # 1. åˆ›å»ºé›†åˆ
            vector_store.create_collection(dimension=settings.embedding_dimension)
            
            # 2. ç´¢å¼•æ–‡æ¡£
            print(f"\nç´¢å¼• {self.num_documents} æ¡æ–‡æ¡£...")
            rag_engine = AdvancedRAGEngine(vector_store, use_parent_child=False)
            
            start_time = time.time()
            rag_engine.index_documents(self.test_documents, show_progress=True)
            index_time = time.time() - start_time
            results["index_time"] = index_time
            
            print(f"âœ“ ç´¢å¼•å®Œæˆï¼Œè€—æ—¶: {index_time:.2f} ç§’")
            
            # 3. æ£€ç´¢æµ‹è¯•
            print(f"\næ‰§è¡Œ {len(self.test_queries)} æ¬¡æ£€ç´¢...")
            search_times = []
            
            for i, query in enumerate(self.test_queries):
                start_time = time.time()
                search_results = rag_engine.search(
                    query=query,
                    top_k=10,
                    enable_hybrid=True
                )
                search_time = time.time() - start_time
                search_times.append(search_time)
                
                print(f"  æŸ¥è¯¢ {i+1}: '{query[:30]}...' - {search_time:.3f}ç§’ - {len(search_results)} ä¸ªç»“æžœ")
            
            total_search_time = sum(search_times)
            avg_search_time = total_search_time / len(search_times)
            
            results["search_time"] = total_search_time
            results["avg_search_time"] = avg_search_time
            
            print(f"\nâœ“ å¹³å‡æ£€ç´¢æ—¶é—´: {avg_search_time:.3f} ç§’")
            
        except Exception as e:
            error_msg = f"è¯„æµ‹å¤±è´¥: {str(e)}"
            print(f"âœ— {error_msg}")
            results["errors"].append(error_msg)
        
        return results
    
    def benchmark_reranking(self):
        """è¯„æµ‹é‡æŽ’åºæ•ˆæžœ"""
        print(f"\n{'='*60}")
        print("è¯„æµ‹é‡æŽ’åºæ•ˆæžœ")
        print(f"{'='*60}")
        
        # ä½¿ç”¨ Chroma è¿›è¡Œæµ‹è¯•ï¼ˆæœ€ç®€å•ï¼‰
        vector_store = ChromaVectorStore("rerank_test")
        vector_store.create_collection(dimension=settings.embedding_dimension)
        
        rag_engine = AdvancedRAGEngine(vector_store, use_parent_child=False)
        
        # åªç´¢å¼•å‰100æ¡æ–‡æ¡£
        test_docs = self.test_documents[:100]
        rag_engine.index_documents(test_docs, show_progress=False)
        
        query = self.test_queries[0]
        print(f"\næµ‹è¯•æŸ¥è¯¢: {query}")
        
        # ä¸å¯ç”¨é‡æŽ’åº
        print("\nã€ä¸ä½¿ç”¨é‡æŽ’åºã€‘")
        results_no_rerank = rag_engine.search(
            query=query,
            top_k=10,
            enable_hybrid=True
        )
        
        print("Top 5 ç»“æžœ:")
        for i, result in enumerate(results_no_rerank[:5]):
            print(f"  {i+1}. [åˆ†æ•°: {result.score:.4f}] {result.document.content[:80]}...")
        
        # å¯ç”¨é‡æŽ’åº
        print("\nã€ä½¿ç”¨ DeepSeek é‡æŽ’åºã€‘")
        results_reranked = rag_engine.rerank(query, results_no_rerank[:10], top_k=5)
        
        print("Top 5 ç»“æžœ:")
        for i, result in enumerate(results_reranked):
            print(f"  {i+1}. [åˆ†æ•°: {result.score:.4f}] {result.document.content[:80]}...")
        
        # æ¸…ç†
        vector_store.drop_collection()
    
    def run_full_benchmark(self):
        """è¿è¡Œå®Œæ•´è¯„æµ‹"""
        print("\n" + "="*60)
        print("RAG ç³»ç»Ÿå®Œæ•´æ€§èƒ½è¯„æµ‹")
        print("="*60)
        
        all_results = []
        
        # è¯„æµ‹ Chromaï¼ˆæœ€å®¹æ˜“è®¾ç½®ï¼Œä¸éœ€è¦é¢å¤–æœåŠ¡ï¼‰
        print("\n\n>>> è¯„æµ‹ Chroma <<<")
        try:
            chroma_store = ChromaVectorStore("benchmark_chroma")
            chroma_results = self.benchmark_vector_store("Chroma", chroma_store)
            all_results.append(chroma_results)
            chroma_store.drop_collection()
        except Exception as e:
            print(f"âœ— Chroma è¯„æµ‹è·³è¿‡: {e}")
        
        # è¯„æµ‹ Qdrantï¼ˆéœ€è¦ Qdrant æœåŠ¡è¿è¡Œï¼‰
        print("\n\n>>> è¯„æµ‹ Qdrant <<<")
        try:
            qdrant_store = QdrantVectorStore("benchmark_qdrant")
            qdrant_results = self.benchmark_vector_store("Qdrant", qdrant_store)
            all_results.append(qdrant_results)
            qdrant_store.drop_collection()
        except Exception as e:
            print(f"âœ— Qdrant è¯„æµ‹è·³è¿‡: {e}")
        
        # è¯„æµ‹ Milvusï¼ˆéœ€è¦ Milvus æœåŠ¡è¿è¡Œï¼‰
        print("\n\n>>> è¯„æµ‹ Milvus <<<")
        try:
            milvus_store = MilvusVectorStore("benchmark_milvus")
            milvus_results = self.benchmark_vector_store("Milvus", milvus_store)
            all_results.append(milvus_results)
            milvus_store.drop_collection()
        except Exception as e:
            print(f"âœ— Milvus è¯„æµ‹è·³è¿‡: {e}")
        
        # æ‰“å°æ±‡æ€»æŠ¥å‘Š
        self._print_summary(all_results)
        
        # è¯„æµ‹é‡æŽ’åºæ•ˆæžœ
        try:
            self.benchmark_reranking()
        except Exception as e:
            print(f"âœ— é‡æŽ’åºè¯„æµ‹å¤±è´¥: {e}")
    
    def _print_summary(self, results: List[Dict]):
        """æ‰“å°è¯„æµ‹æ±‡æ€»"""
        print("\n\n" + "="*60)
        print("è¯„æµ‹æ±‡æ€»æŠ¥å‘Š")
        print("="*60)
        
        if not results:
            print("æ²¡æœ‰æˆåŠŸå®Œæˆçš„è¯„æµ‹")
            return
        
        print(f"\næµ‹è¯•é…ç½®:")
        print(f"  - æ–‡æ¡£æ•°é‡: {self.num_documents}")
        print(f"  - æŸ¥è¯¢æ•°é‡: {len(self.test_queries)}")
        print(f"  - å‘é‡ç»´åº¦: {settings.embedding_dimension}")
        
        print(f"\næ€§èƒ½å¯¹æ¯”:")
        print(f"{'æ•°æ®åº“':<15} {'ç´¢å¼•æ—¶é—´(ç§’)':<15} {'å¹³å‡æ£€ç´¢æ—¶é—´(ç§’)':<20}")
        print("-" * 60)
        
        for result in results:
            if not result.get("errors"):
                print(f"{result['name']:<15} {result['index_time']:<15.2f} {result['avg_search_time']:<20.3f}")
        
        # æ‰¾å‡ºæœ€å¿«çš„
        if results:
            fastest_index = min(results, key=lambda x: x.get('index_time', float('inf')))
            fastest_search = min(results, key=lambda x: x.get('avg_search_time', float('inf')))
            
            print(f"\nðŸ† ç´¢å¼•é€Ÿåº¦æœ€å¿«: {fastest_index['name']} ({fastest_index['index_time']:.2f}ç§’)")
            print(f"ðŸ† æ£€ç´¢é€Ÿåº¦æœ€å¿«: {fastest_search['name']} ({fastest_search['avg_search_time']:.3f}ç§’)")


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºè¯„æµ‹å®žä¾‹
    benchmark = RAGBenchmark(num_documents=1000)
    
    # è¿è¡Œå®Œæ•´è¯„æµ‹
    benchmark.run_full_benchmark()


if __name__ == "__main__":
    main()
